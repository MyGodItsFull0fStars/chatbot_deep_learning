@book{texbook,
  author = {Donald E. Knuth},
  year = {1986},
  title = {The \TeX{} Book},
  publisher = {Addison-Wesley Professional}
}

@ARTICLE{feedforwardsize,
  author={Bebis, G. and Georgiopoulos, M.},
  journal={IEEE Potentials}, 
  title={Feed-forward neural networks}, 
  year={1994},
  volume={13},
  number={4},
  pages={27-31},
  doi={10.1109/45.329294}
}

@misc{chatbottensorflow,
  author       = {George Kassabgi},
  howpublished = {Chatbots Magazine},
  title        = {Contextual Chatbots with Tensorflow},
  year         = {2017}
}

@article{abdul2015survey,
  title={Survey on chatbot design techniques in speech conversation systems},
  author={Abdul-Kader, Sameera A and Woods, John C},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={6},
  number={7},
  year={2015},
  publisher={The Science and Information (SAI) Organization}
}

@misc{rosebrockPytorchFirstCNN,
  author       = {Adrian Rosebrock},
  howpublished = {Online - pyimagenet},
  title        = {Pytorch: Training your first Convolutional Neural Network},
  year         = {2021}, 
  note         = {Available at \url{https://www.pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn}},
}

@misc{rosebrockPytorchWithPretrainedModel,
  author       = {Adrian Rosebrock},
  howpublished = {Online - pyimagenet},
  title        = {Pytorch image classification with pre-trained networks},
  year         = {2021}, 
  note         = {Available at \url{https://www.pyimagesearch.com/2021/07/26/pytorch-image-classification-with-pre-trained-networks}},
}

@misc{pytorchCNNTrainingAClassifier,
  author       = {PyTorch},
  howpublished = {Online - pytorch.org},
  title        = {Training a Classifier},
  year         = {2021}, 
  note         = {Available at \url{https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}},
}

@misc{koehrsenPrecisionRecall,
  author       = {Will Koehrsen},
  howpublished = {Online - buildin.com},
  title        = {When Accuracy Isn't Enough, Use Precision and Recall to Evaluate Your Classification Model},
  year         = {2021},
  note         = {Available at \url{https://builtin.com/data-science/precision-and-recall}},
}

@misc{googleClassificationAccuracy,
  author       = {Google},
  howpublished = {Online - developers.google.com},
  title        = {Classification: Accuracy},
  year         = {2020},
  note         = {Available at \url{https://developers.google.com/machine-learning/crash-course/classification/accuracy}},
}

@article{squadPaper,
       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},
                 Konstantin and {Liang}, Percy},
        title = "{SQuAD: 100,000+ Questions for Machine Comprehension of Text}",
      journal = {arXiv e-prints},
         year = 2016,
          eid = {arXiv:1606.05250},
        pages = {arXiv:1606.05250},
archivePrefix = {arXiv},
       eprint = {1606.05250},
}

@misc{squadExample,
  author       = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},
                 Konstantin and {Liang}, Percy},
  howpublished = {Online - https://rajpurkar.github.io/SQuAD-explorer},
  title        = {Prime\_number - The Stanford Question Answering Dataset},
  year         = {2016},
  note         = {Available at \url{https://rajpurkar.github.io/SQuAD-explorer/explore/}}
}

@misc{bertTorwardsDataScience,
  author       = {James Briggs},
  howpublished = {Towards Data Science},
  title        = {How to Train Bert For Q\&A in Any Language},
  year         = {2021},
  note         = {Available at \url{https://towardsdatascience.com/how-to-train-bert-for-q-a-in-any-language-63b62c780014}}
}

@misc{innerProject,
  author       = {Christian Bauer},
  howpublished = {bert\_squad directory},
  title        = {Transforming Text Comprehension},
  year         = {2022}
}

@misc{allYouNeedBERT,
  author       = {Raman Kumar},
  howpublished = {Online - Analytics Vidhya},
  title        = {All You Need to know about BERT},
  year         = {2021},
  note         = {Available at \url{https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/}},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{understandingAttention,
  author       = {Nikhil Agrawal},
  howpublished = {Online - Medium},
  title        = {Understanding Attention Mechanism: Natural Language Processing},
  year         = {2020},
  note         = {Available at \url{https://bit.ly/34GbeFq}},
}


@misc{autoregressiveGeorge,
  author       = {George Ho},
  howpublished = {Online - www.goergeho.org},
  title        = {Autoregressive Models in Deep Learning - A Brief Survey},
  year         = {2019},
  note         = {Available at \url{https://www.georgeho.org/deep-autoregressive-models/}},
}

@misc{wandbScreenshot,
  author       = {Christian Bauer},
  howpublished = {Online - Weights \& Biases},
  title        = {Golden Water Training Metric},
  year         = {2022},
  note         = {Available at \url{https://wandb.ai/my-god-its-full-of-stars/BERT%20Training/runs/10kfa9lk?workspace=user-my-god-its-full-of-stars}},
}