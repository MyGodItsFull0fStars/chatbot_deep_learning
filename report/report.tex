\input{settings} % add packages, settings, and declarations in settings.tex
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}

\title{Transforming Text Comprehension}
\author{Christian Bauer, 01560011}
\date{}

% \bibliographystyle{plain}
% \addbibresource{./resources/references.bib}

\begin{document}


\maketitle



% Report

% A report should be a 6-8 pages long document structured as a research paper, as shown below. You may modify this structure, but the suggested list is a good starting point.

\setcounter{tocdepth}{2}
\tableofcontents

\pagebreak

    \section{Motivation}
    \label{sec:motivation}

    % Motivation: describe the problem you are solving, explain why is it important, and provide a short summary of your methods and obtained results.

        % NLP is a hard task for computers and writing a chatbot that "understands" user input, meaning providing a satisfactory answer is a challenging task.
        % For the project, a chatbot will be implemented, which should be able to answer questions from the \emph{Stanford Question Answering Dataset} (short= \emph{SQuAD})\footnote{SQuAD source: \url{https://rajpurkar.github.io/SQuAD-explorer}}.
        % This dataset was extracted by volunteers with over 500 Wikipedia articles. 
        % Each of these articles is considered a title in the dataset and each holds numerous \emph{question-answer-sets (=QAS)}.
        % This SQuAD-dataset will be the source for the training dataset used to train the machine learning models.
        Natural Language Processing (NLP) is a hard task for computers and creating a system that is able to "comprehend" natural language to be able to provide a reasonable answer to questions is very challenging and a lot of research is done at the moment to improve NLP tasks.
        The first idea for this project was to implement a chatbot that will be build on top of a Feed Forward Neural Network (FNN).
        While researching resources for that task, a more interesting task was found, the comprehension of text with the help of deep learning.

        The task of natural language comprehension was a more challenging approach, since opposed to the chatbot using FNN that was used similar to a lookup table, finding the correct answer to a question given some context was a daring and unknown field to me.
        This comprehension is already widely used by companies such as Google, and often when you look for a topic, you get a well-defined short summary presented above all query objects.
        Creating NLP-models that are able to comprehend text at a human reader level is a goal for a long time in the computer science field and while there are already very impressive models available, this level of text comprehension is yet not achieved.
        
        For this project, the text comprehension neural networks called \texttt{BERT} is being used.
        BERT reaches impressive amounts of text comprehension in competitions such as the competition using the \emph{Stanford Question Answering Dataset (SQuAD)}.



    \section{Data}
    \label{sec:data}
    % Data: describe the data you are using in the project and provide examples. what kind of data you are using, where does it come from? what are the properties of your data, like balance, missing values, scale, etc., and what are you going to do about it? what kind of preprocessing, filtering, augmentation, or other manipulations are you applying in the project?  

        For the project, "The Stanford Question Answering Dataset (SQuAD)"\footnote{Source: https://rajpurkar.github.io/SQuAD-explorer/} will be used. This dataset was generated for reading comprehension training and the data is based on Wikipedia articles.
        There exist two versions of this dataset, \texttt{SQuAD 1.1} that holds 100,000 question-answer sets, and \texttt{SQuAD 2.0} that contains all question-answer sets from \texttt{SQuAD 1.1} and additionally also includes 50,000 question-answer sets that are unanswerable, and a modern reading comprehension system has to be able to also determine whether a question is answerable or not.


        \subsection{Example Data Element of SQuAD}
        \label{subsec:data-example}

            As an example, a section from the \texttt{Prime\_number} data chunk was taken as can be seen in figure \ref{fig:-squad-example}. On the left, the \emph{context} is shown, which is the text paragraph that contains the answer at some position.
            In the right column of the figure, the question is presented in bold letters, and the ground truth answer that is to be found is highlighted in green. \emph{Note: This is only partially represents the actual data that is provided by SQuAD.}

            \begin{figure}[h!]
                \centering
                \caption{SQuAD Example Question-Answer Set\cite{squadExample}}
                \includegraphics[width=0.95\textwidth]{figures/squad_example.png}
                \label{fig:-squad-example}
            \end{figure}

            The dataset is provided by the company \emph{Hugging Face}\footnote{URL: https://huggingface.co/}
            This company provides a vast amount of datasets and (pretrained) machine learning models. 
            Those are also used in this project and are downloaded by using the Python library \texttt{datasets} created by Hugging Face.

            \begin{figure}
                \centering
                \caption{Usage of the \texttt{datasets} library\cite[\texttt{data\_properties.ipynb}]{innerProject}}
                \includegraphics[scale=0.8]{figures/datasets_library.png}
                \label{fig:-datasets-library}
            \end{figure}

            With the help of the code in figure \ref{fig:-datasets-library} the train- and validation datasets can be downloaded.
            Each set consists of the following components:
            \begin{itemize}
                \item \textbf{Id:} The unique identifier of the SQuAD element.
                \item \textbf{Title:} The title of the Wikipedia entry the SQuAD element is based on.
                \item \textbf{Question:} A string containing the question the model gets asked.
                \item \textbf{Context:} A larger text sequence that contains the answer to the question.
                \item \textbf{Answer:} The part of the context, that answers the question as well as the start position of the answer in the context.
            \end{itemize}

            \begin{figure}
                \centering
                \caption{SQuAD Train Dataset Example\cite[\texttt{bert\_training.ipynb}]{innerProject}}
                \includegraphics[width=0.99\textwidth]{figures/train_dataset_example.png}
                \label{fig:-train-dataset-example}
            \end{figure}

        \subsection{Data Properties}
        \label{subsec:-data-properties}

            The data properties were investigated with the focus of rather general values, like number of data entries in total, missing values and the balance of the dataset. All the figures used in this section are taken from the Jupyter file \texttt{data\_properties.ipynb} that can be found in the \texttt{bert\_squad} package.
            
        % TODO what are the properties of your data, like balance, missing values, scale, etc.
            \subsubsection{General Data Properties and Missing Values}
            \label{subsubsection:-data-properties---missing-values}
                The dataset consists of a total of $98169$ SQuAD elements that are split into a test dataset and validation dataset. The training dataset consists of $87599$ SQuAD elements and the validation dataset consists of $10570$.

                \begin{wrapfigure}{R}{0.5\textwidth}
                    \caption{Missing Values}
                    \begin{center}
                        \includegraphics[width=0.49\textwidth]{figures/data_prop_no_missing_elements.png}
                    \end{center}
                    \label{fig:-data-properties---no-missing-values}
                \end{wrapfigure}

                The dataset is complete in the sense that no dataset element is missing a field value. This was tested in the Jupyter file \texttt{bert\_squad/data\_properties.ipynb} and can be seen in figure \ref{fig:-data-properties---no-missing-values}.

            
            \subsubsection{Data Balance of SQuAD}
            \label{subsubsection:-data-properties---balance-of-data}

                The dataset itself is quite unbalanced, with the maximum being a total of $817$ entries and the minimum being only $22$ Question Answer entries. 
                The mean over the entire training dataset being $198.18778$ entries per title.
                In figure \ref{fig:-data-properties---balance-of-data-barplot} a bar plot is used over all values of the training dataset to display the imbalance of the dataset visually.
                As can be seen in the graph, the number of entries for each data entry varies a lot over the overall training dataset.

                \begin{figure}[h!]
                    \centering
                    \caption{Balance of SQuAD}
                    \includegraphics[width=0.95\textwidth]{figures/balance_of_squad_dataset.png}
                    \label{fig:-data-properties---balance-of-data-barplot}
                \end{figure}


            \subsubsection{Answer Position Off by One or Two Positions}
            \label{subsubsection:-data-properties---incorrect-answer-positions}

                In the used reference "How to Train Bert For Q\&A in Any Language"\cite{bertTorwardsDataScience} it was stated, that 
                some SQuAD answer positions might be off by one or two tokens, and this is corrected in the function called \texttt{add\_end\_idx} (further discussed in section \ref{section:-preparing-data-with-add-end-idx}).
                While investigating the dataset for the occurrence of such misaligned answers, none could be found. 
                The test of this was quite simple, there happens a check for a misaligned answer position inside the code, and for the investigation every SQuAD element was added to a list, and since this list was still empty after both the training-, and validation dataset were preprocessing with the \texttt{add\_end\_idx} function, no misaligned elements either exist or couldn't be found by this function. 
                This leads to the assumption, that this statement either refers to another dataset, the dataset provided by huggingface was corrected in the meantime, or that this was a wrong assumption by the author.

% \pagebreak
        \subsection{Data Preprocessing/Augmenting}
        \label{subsec:-data-preprocessing-augmenting}

            Multiple steps of data preprocessing or data augmentation were used to be able to use the dataset for the machine learning approach used. 
            Those steps are mentioned in this section according to the sequence in the \texttt{bert\_training.ipynb} they are actually used.

            \subsubsection{Preprocessing with Prepare Data Function}
            \label{subsubsec:-preparing-data-with-prep-data}
            
                \begin{wrapfigure}{r}{0.40\textwidth}
                    \caption{The Function \texttt{prep\_data}}
                    \begin{center}
                        \includegraphics[width=0.3\textwidth]{figures/prep_data_function.png}
                    \end{center}
                    \label{fig:-preparing-data-with-prep-data-function}
                \end{wrapfigure}
                With the help of the function \texttt{prep\_data}, all irrelevant fields get omitted, such as \texttt{id, title}, since they will not be used in the training of the ML-model.
                This function also calls another function that is responsible for adding the answer for every question in the proper manner, so they can be used for the training. This is further explained in the section \ref{section:-preparing-data-with-add-end-idx}.
                The \texttt{prep\_data} function will be used on both the training dataset and the validation dataset before further preprocessing.


            \subsubsection{Preprocessing with \texttt{add\_end\_idx}}
            \label{section:-preparing-data-with-add-end-idx}

                The function \texttt{add\_end\_idx} is used to make later manipulations of the answer field of each SQuAD entry easier, as well as adding the inner field \texttt{answer\_end} to it, that denotes the end index position of the \emph{gold text} (gold text denotes the answer we are expecting in the context) found in the context.
                \begin{wrapfigure}{R}{0.6\textwidth}
                    \caption{The Function \texttt{add\_end\_idx}}
                    \begin{center}
                        \includegraphics[width=0.5\textwidth]{figures/add_end_idx_function.png}
                    \end{center}
                    \label{fig:-data-preprocessing-augmenting---preparing-data-with-add-end-idx-function}
                \end{wrapfigure}
                The inner fields \texttt{'text', 'answer\_start'} both hold a list of data, each with one element, the first holding the answer text, the latter holding an integer element, that denotes the start position of the answer text. To make later manipulations and retrieval of those values easier, the fields simply will hold the single value of each list instead of the list with only one element.
                In the next step, the end index position of the gold text will be added, which is simply the start index position plus the length of the gold text.

                Next, with an index slice plus a check if the answer start and end index position inside the context match the gold text.
                As stated in \ref{subsubsection:-data-properties---incorrect-answer-positions}, the author of the reference "How To Train Bert Q\&A in Any Language"\cite{bertTorwardsDataScience} mentioned that the positions of the indices can be misaligned for some data entries. 
                This could not be verified with the provided code and some tests, but this part was still kept in the code in case this was referring to the SQuAD 2.0 dataset, which was not used at for now.
                After those steps, the answers given to this function will be replaced by this augmented answer dictionary.


            \subsubsection{Tokenization}
            \label{par:-data-preprocessing-augmenting---tokenization}

                To be able to feed the dataset to the ML-model BERT (see \ref{sec:-bert}), a tokenization process is necessary.
                For this the tokenizer \texttt{BertTokenizerFast} provided by the huggingface package \texttt{transformers} is used.
                This tokenizer can be used for both the \texttt{context} and \texttt{question} fields of all SQuAD entries and encodes each of them to an array of single tokens. 
                As an example, the sentence "Computer Vision research has made a giant leap since AlexNet" will be tokenized to "[CLS] computer vision research has made a giant leap since alexnet [SEP]" after decoding.
                Those encoded arrays called \texttt{input\_ids} contain special tokens required by BERT to properly understand the provided input. These arrays also contain the input ID for every word provided as an input to the tokenizer.
                This can be seen in the figure \ref{fig:-tokenizer-output-decode} in the first cell, in which the example input is tokenized and a dictionary with the field \texttt{input\_ids} contains the input IDs for every word.
                \begin{figure}[h!]
                    \centering
                    \caption{Tokenization of Input and Decoding To String \cite[\texttt{bert\_training.ipynb}]{innerProject}}
                    \includegraphics[width=0.95\textwidth]{figures/tokenizer_output_decode.png}
                    \label{fig:-tokenizer-output-decode}
                \end{figure}
                \emph{Note: The used tokenizer is already pretrained for the usage of text comprehension and doesn't need additional training steps.}


                \paragraph{The Classification Token}
                \label{par:-the-classification-token}

                    \texttt{[CLS]} is a special classification token and the last hidden state of BERT corresponding to this token ($h_{[CLS]}$) is used for classification tasks. \cite[Section - Model Overview]{allYouNeedBERT}


                \paragraph{The Separator Token}
                \label{par:-the-separator-token}

                    The separator token \texttt{[SEP]} has to be put at the end of a single input.
                    When a task requires more than one input such as NLI and Q-A tasks, \texttt{[SEP]} token helps the model to understand the end of one input and the start of another input in the same sequence input \cite[Section - Model Overview]{allYouNeedBERT}.


            \subsubsection{Adding Token Positions}
            \label{par:-data-preprocessing-augmenting---adding-token-positions}

                Since tokens get fed into the ML-model, the start and end positions of the tokens also need to be provided.
                This is done with the help of the function \texttt{add\_token\_positions} found in the \texttt{bert\_training.ipynb} file of the \texttt{bert\_squad} package.

\pagebreak
    \section{Theoretical Part}
    \label{sec:theoretical-part}
            
    % Theoretical part: literature overview: which models from the literature are suitable for this problem given your data and why? which algorithms can be used to train them? how these models are related to your hypothesis? present an overview of your approach: what steps are executed in the learning pipeline? which hyperparameters of the learning algorithm are available and how they can influence the results? You must show that you applied methods considered during the semester to the selected problem. Feel free to include any figures or tables helping to describe your method and compare it with others. If you use information from other sources, please cite it properly.

    
    
    \subsection{Attention}
    \label{sec:-attention}
    
        Attention is an important building block inside the Transformers model architecture (see \ref{sec:-transformers}).
        It was developed to increase the performance of encoder-decoder Recurrent Neural Network (RNN) models.
        One of the drawbacks of using encoder-decoder architectures was the fixed input vector length which will be decoded to an output at each step.
        This decoding step might result in issues, if the neural network has to decode an input that consists of long sentences, especially those longer than the ones used to train the model.

        The basic idea of Attention is that the model will try to predict an output word based on the most relevant information of the input, so it only takes parts of the given input into account instead of the whole sentence.
        This means, attention tries to give more importance to a few input words.

        Three types of attention layers are used in BERT, the "Self Attention", "Scaled Dot-Product Attention" and the "Multi Head Attention".
        A short introduction to all three concepts can be found at "Understanding Attention Mechanism: Natural Language Processing" \cite{understandingAttention}.

    \begin{wrapfigure}{R}{0.35\textwidth}
        \caption{The Transformer - model architecture \cite[Figure 1 on page 3]{vaswani2017attention}}
        \begin{center}
            \includegraphics[width=0.3\textwidth]{figures/transformer_model_architecture.png}
        \end{center}
        \label{fig:-transformer-architecture}
    \end{wrapfigure}
    
    \subsection{Transformers}
    \label{sec:-transformers}
    
        Attention finds its application in the concept of Transformers. 
        In Transformers, attention is used in different ways.
        In the Transformers "encoder-decoder attention" layers (see figure \ref{fig:-transformer-architecture}) they are used to mimic the typical encoder-decoder behavior, the encoder uses self-attention layers to attend all positions in the previous layer of the encoder, and similarly this is used for the decoder as well to attend all positions.
        For this architecture, it is required to use a softmax to mask out all values of the input that correspond to illegal connections to preserve auto-regressive properties.

        \begin{quote}
            Put simply, \textbf{an autoregressive model is merely a feed-forward model which predicts future values from past values.} \cite{autoregressiveGeorge}
        \end{quote}
    
    
    \subsection{BERT}
    \label{sec:-bert}

        \textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentation from \textbf{T}ransformer, often abbreviated to BERT heavily relies on the concept of \emph{Attention} and the usage of \emph{Transformers} that are presented in the famous paper "Attention is All You Need"\cite{vaswani2017attention}.
        This is the machine learning architecture that is chosen for the text comprehension of SQuAD.
        BERT was able to surpass the human benchmark for the SQuAD dataset with the human annotators achieving an exact match score of $82.304\%$ and an F1-score of $91.221$ and the original BERT model achieved an exact match score of $85.083\%$ and a F1-score of $91.835\%$ .
        Inside the ML-model, there is a multilayer bidirectional transformer encoder structure, hence the name of BERT and in general it is differentiated between two types of BERT models, that are named BERT$_{Base}$ and BERT$_{Large}$ \cite[Values taken from]{humanVSBert}.
        The BERT$_{Base}$ model has 12 layers of transformer blocks, a hidden size of $768$ and 12 self-attention heads as well as 110 million trainable parameters. \textbf{Note: This is the model type I used in this project.}

        BERT$_{Large}$ is even bigger, using $24$ layers of transformer blocks and has a hidden size of $1024$, it uses 16 self-attention heads and has $340$ million trainable parameters. This of course is too big for a regular computing device to retrieve a feasible result.
        BERT can be used for a multitude of NLP problems, such as Question Answering (what I did in this project), Neural Machine Translation, Sentiment Analysis and Text Summarization.
   

        % TODO pic of BERT
        A completely untrained BERT model needs to pre-train on two unsupervised tasks ("Masked Language Model" (MLM) and "Next Sentence Prediction" (NSP)) before it can be used for the text comprehension task to retrieve the correct answer of a SQuAD entry. 
        
        \begin{enumerate}
            \item \textbf{Masked Language Model (MLM)}
            \begin{itemize}
                \item In this task, BERT will mask random words in a sentence and try to find the correct words, that are masked.
                \item Example: The \texttt{[MASK1]} brown fox \texttt{[MASK2]} over the lazy dog should result in returning: \texttt{[MASK1]} = quick and \texttt{[MASK2]} = jumped.
            \end{itemize}

            \item \textbf{Next Sentence Prediction (NSP)}
            \begin{itemize}
                \item BERT is provided with two sentences as an input and tries to determine if the second sentence is a correct successor to the first sentence.
                \item Example: \texttt{Sentence A}: Does Bertram live in Atlanta? \texttt{Sentence B}: Yes, he lives there. The correct answer should be \texttt{True}, \texttt{Sentence B} follows \texttt{Sentence A}.
            \end{itemize}
        \end{enumerate}
        These two unsupervised learning tasks are only briefly mentioned, since the used BERT ML-model was already pretrained with these tasks beforehand and only needs to be trained on the SQuAD training set for this project.
        After this pretraining, the model is ready for the SQuAD dataset.
        Now the context and question of the SQuAD entries will be provided to the model, and the returned positions of the start and end index are the most probable position of the answer according to the current BERT model.
        With the help of the \texttt{AdamW} optimizer, these provided positions will be improved over time. 
        % https://blog.scaleway.com/understanding-text-with-bert/

                
\pagebreak
    \section{Implementation}
    \label{sec:implementation}
            
    % Implementation: how did you implement your approach? which tools were used? show interesting snippets or present your algorithms.

        \subsection{The Tools of the Project}
        \label{sec:-tools-of-the-project}
            The approach was implemented with the help of PyTorch and huggingface.
            Huggingface was heavily used in this project, the BERT$_{Base}$ model as well as the corresponding tokenizer and the optimizer \texttt{AdamW} were all retrieved from the transformers package, huggingface provides. 
            PyTorch is used for the deep learning tasks and provides many useful tools for handling tensors efficiently. 
            PyTorch also provides a scheduler, that is used to regulate the optimizer of the huggingface transformer package.
            This is done to assure that the chosen learning rate will be beneficial while the training commences.

            The Weights \& Biases package \texttt{wandb} was used to track metrics of each training run, such as accuracy or the f1-score of the currently trained model.
            One training run is displayed in the figure \ref{fig:-wandb-metric}.

        \subsection{Training Loop}
        \label{sec:-training-loop}
            The main training is done in the function \texttt{train\_model}, that takes the start and end epoch, as well as model, optimizer and scheduler as parameters.
            This function includes the section for the training, as well as the validation step of the training progress.
            Before the training section starts, it is made sure that the model is set to training mode with \texttt{model.train()}. This makes sure the weights of the model will be trained in the progress.
            Next, all metric values get initialized and set to initial values to assure that each training epoch metric has the same initial point and is representative.

        \subsection{Creating a Partial Dataloader}
        \label{sec:-creating-a-partial-dataloader}
            For the training, a helper function called \texttt{get\_next\_dataloader} (see figure \ref{fig:-get-next-dataloader}) is introduced, that only loads the dataset partially and converts it do a dataloader instance. This is done, because loading the entire training dataset was not possible, since it was too large to be loaded all at once into the memory.
            This behavior was double-checked with the help of the monitoring tool \texttt{htop} where it could be seen that the whole memory was being allocated before the training started to crash.

            \begin{figure}[h!]
                \centering
                \caption{The Get Next Dataloader Function \cite[\texttt{bert\_training.ipynb}]{innerProject}}
                \includegraphics[width=0.95\textwidth]{figures/get_next_dataloader.png}
                \label{fig:-get-next-dataloader}
            \end{figure}
        This function takes a small portion of the dataset, and applies all data augmentation steps mentioned in the section \ref{subsec:-data-preprocessing-augmenting} to this chunk and initializes a \texttt{DataLoader} with it, that will be returned to the training loop as the next batch to train on.
        The first step inside the training loop is to zero the gradients of the optimizer so the tracking of the gradients and the progress of the model are done correctly.

        \subsection{Inside the Main Training Loop}
        \label{sec:-inside-the-main-training-loop}
            Next, all required batch values will be put onto the training device. If \texttt{cuda} is available, this option will always be chosen to train the model on it.
            Once this is done, the batch values are used as the parameters for the ML-model to train on.
            The output of the model is then used for the metrics \textbf{accuracy, f1 start, f1 end}.
            Then the loss gets retrieved and the backpropagation of the loss and the optimizer weights is done.
            The loss function \texttt{AdamW} was chosen as an optimizer, since it is highly popular in literature and often used for such tasks as text comprehension with SQuAD.
            The acquired metric values then get logged to \texttt{wandb} to compare them with other runs and to track the current progress graphically.
            \emph{Note: The validation section is similar to the training section, but it is made sure that the gradients will not be changed, and the model is switched to evaluation mode.}
            After each episode, the model, optimizer and scheduler are stored on the disk with the help of the function \texttt{torch.save()} function.

        \subsection{Loading Model Weights from Disk}
        \label{sec:-loading-model-weights-from-disk}
            For making sure that the training process can continue at a later stage, the functions \texttt{get\_last\_saved\_epoch()} and \texttt{init\_model()} can be used to automatically load the last trained model, optimizer and scheduler from disk space.

        \subsection{Dealing with Memory Issues}
        \label{sec:-dealing-with-memory-issues}
            Since there were memory leaks or cached memory problems occurring during the training of the model, several measurements had to be taken, to successfully train the model for a longer period of time without running out of memory. One of those measures was already mentioned, namely the partitioning of the training dataset and loading small chunks of data into the memory.
            But this was not sufficient to fully omit those crashes. That is why numerous variables were removed and then reinitialized during the training, and this actually helped to make the training possible.
            Later on, a method called \texttt{torch.cuda.empty\_cache()} was found, that may have made those steps redundant.
        

    \section{Evaluation}
    \label{sec:evaluation}

    % Evaluation: present and visualize results of your evaluation, explain what do your results mean, why was your approach successful, why not? compare it to some baseline either implemented yourself or from the literature, blogs, Kaggle, etc.
        For the evaluation, several metrics were used to see the progress the training made. Training accuracy and validation accuracy denote how many answers were correctly given for each context and question. The loss values for training and validation correlate with the accuracy, meaning that if the accuracy increases, the loss decreases in the same epoch. This is only natural to happen.
        
        Also, another metric was chosen to further test the \emph{F1-score}. This is also regarded as the \emph{harmonic mean} of Precision and Recall. It was chosen as another metric since the F1-score better measures the incorrectly classified cases than the Accuracy metric. 
        The F1-score was measured for both the start position of an answer and the end position separately to see how those predictions change over time.

        \begin{tcolorbox}
            \textbf{F1-score}
            \textit{The harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy metric.}
            $$\text{F1-score} = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$
        \end{tcolorbox}
   
        \begin{figure}[h!]
            \centering
            \caption{Weights \& Biases Metric of Training Run \cite{wandbScreenshot}}
            \includegraphics[width=0.95\textwidth]{figures/wandb_metrics.png}
            \label{fig:-wandb-metric}
        \end{figure}
        The mentioned metrics are shown in the figure \ref{fig:-wandb-metric}.
        As can be seen, the f1-scores for both the start and end position of the answer highly alternate. The accuracy and loss both are not improving in the shown graph, and it is highly likely, that many more training epochs are required, to show actual improvements in the performance of the model.
        Comparing those values to the Kaggle leaderboard with values as high as approximately $97\%$\footnote{Kaggle Leaderboard for Bert: \url{https://www.kaggle.com/c/bert-classification/leaderboard}} for BERT-classification, these results are rather disappointing in the current state and more training would be required to get closer to satisfying values.

        % TODO Add evaluation with full BERT

\pagebreak
    \section{Conclusion}
    \label{sec:conclusion}
    % Conclusions: how can you evaluate the results of your work? what would you recommend for future steps?

        The project and the challenges I faced while implementing and researching ways to achieve text comprehension were intriguing. 
        Some of the largest issues I faced was that most literature only partially presents their findings in practical manners, or online content even was incorrect (code could not be computed because they used different versions of their code for explanation) and this had to be taken care of.

        The biggest issue that I want to make aware of, is the huge amount of memory, that an ML-model such as BERT requires in order to function properly. This starts with disk space, since each epoch that gets stored on disk, requires about 1.3 GB space on it (\emph{Note: This result is an approximation of the disk space needed for the model, scheduler and optimizer for each time it gets stored}).
        Also, the working memory has to be at least 16 GB, so the process will not run out of memory while the training.
        The last big issue memory wise is that I often ran out of VRAM many times during the training, which was a result of cached memory fragments that started to add up and crashed the entire process.

        Since the entire process was slowed down a lot just because the required memory was too much, my personal recommendation is to use \texttt{DistilBert} instead of a full BERT ML-model for text comprehension if the used machine for the training isn't really powerful.

    \section{Supplementary Materials}
    \label{sec:supplementary-materials}

        The link to the trained BERT-model, optimizer and scheduler can be found at the OneDrive link: \url{https://1drv.ms/u/s!Apv-VJLmVPuUkYxG_Ubom92bnRqCIQ?e=TChQl6}. 
        These PyTorch files have to be added to the \texttt{bert\_squad/models/} directory.

    % Supplementary materials: The report must be submitted as an archive comprising all code and artifacts (data, custom python modules, images, videos, etc.) required for its correct representation, evaluation, and exemplification of your work. If data is too large for a submission, please provide a link where this data can be downloaded from.

    \pagebreak

    \bibliography{references.bib}
    \bibliographystyle{ieeetr}
    % \printbibliography

\end{document}
